{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18824,"status":"ok","timestamp":1655459885151,"user":{"displayName":"Quốc Hưng Vũ","userId":"18294274047591865356"},"user_tz":-420},"id":"NZ5WuRG1rRGv","outputId":"6c74cd8f-e93d-4937-da50-b07c69c6ea9e"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1655459885151,"user":{"displayName":"Quốc Hưng Vũ","userId":"18294274047591865356"},"user_tz":-420},"id":"lmkwq5mXrfS7"},"outputs":[],"source":["import os\n","os.chdir(\"C:/Users/HP/Desktop/Transformer-OCR\")"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":5822,"status":"ok","timestamp":1655459890969,"user":{"displayName":"Quốc Hưng Vũ","userId":"18294274047591865356"},"user_tz":-420},"id":"xV4zBcBhq067"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import time\n","from mydataset import MyDataset\n","from mydataset import char2token, vocab, token2char, subsequent_mask\n","from mydataset import Batch\n","from model import make_model\n","import data_preprocessing\n","from PIL import Image\n","import numpy as np\n","from torchvision.transforms import ToTensor\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","from tqdm import tqdm\n","import cv2\n","import sys, os\n","from torchvision import transforms"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1655459890971,"user":{"displayName":"Quốc Hưng Vũ","userId":"18294274047591865356"},"user_tz":-420},"id":"3WaMpeIeq3gu","outputId":"053bc417-81ec-4254-d78c-c4647785550c"},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]}],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(device)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1655459890971,"user":{"displayName":"Quốc Hưng Vũ","userId":"18294274047591865356"},"user_tz":-420},"id":"gsS7kYEXrAnK"},"outputs":[],"source":["class NoamOpt:\n","    \"Optim wrapper that implements rate.\"\n","    def __init__(self, model_size, factor, warmup, optimizer):\n","        self.optimizer = optimizer\n","        self._step = 0\n","        self.warmup = warmup\n","        self.factor = factor\n","        self.model_size = model_size\n","        self._rate = 0\n","        \n","    def step(self):\n","        \"Update parameters and rate\"\n","        self._step += 1\n","        rate = self.rate()\n","        for p in self.optimizer.param_groups:\n","            p['lr'] = rate\n","        self._rate = rate\n","        self.optimizer.step()\n","        \n","    def rate(self, step = None):\n","        \"Implement `lrate` above\"\n","        if step is None:\n","            step = self._step\n","        return self.factor * \\\n","            (self.model_size ** (-0.5) *\n","            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n","        \n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1655459890972,"user":{"displayName":"Quốc Hưng Vũ","userId":"18294274047591865356"},"user_tz":-420},"id":"vb9pVOoKrDuI"},"outputs":[],"source":["class LabelSmoothing(nn.Module):\n","    \"Implement label smoothing.\"\n","    def __init__(self, size, padding_idx=0, smoothing=0.0):\n","        super(LabelSmoothing, self).__init__()\n","        self.criterion = nn.KLDivLoss(size_average=False)\n","        self.padding_idx = padding_idx\n","        self.confidence = 1.0 - smoothing\n","        self.smoothing = smoothing\n","        self.size = size\n","        self.true_dist = None\n","        \n","    def forward(self, x, target):\n","        assert x.size(1) == self.size\n","        true_dist = x.data.clone()\n","        true_dist.fill_(self.smoothing / (self.size - 2))\n","        true_dist.scatter_(1, target.data.unsqueeze(1).type(torch.int64), self.confidence)\n","        true_dist[:, self.padding_idx] = 0\n","        mask = torch.nonzero(target.data == self.padding_idx)\n","        if mask.dim() > 0:\n","            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n","        self.true_dist = true_dist\n","        return self.criterion(x, Variable(true_dist, requires_grad=False))"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1655459890972,"user":{"displayName":"Quốc Hưng Vũ","userId":"18294274047591865356"},"user_tz":-420},"id":"mM1E0WlerGWt"},"outputs":[],"source":["class SimpleLossCompute:\n","    \"A simple loss compute and train function.\"\n","    def __init__(self, generator, criterion, opt=None):\n","        self.generator = generator\n","        self.criterion = criterion\n","        self.opt = opt\n","        \n","    def __call__(self, x, y, norm):\n","        x = self.generator(x)\n","        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), \n","                              y.contiguous().view(-1)) / norm\n","        if self.opt is not None:\n","            loss.backward()\n","            self.opt.step()\n","            self.opt.optimizer.zero_grad()\n","        return loss.data * norm\n"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1655459890973,"user":{"displayName":"Quốc Hưng Vũ","userId":"18294274047591865356"},"user_tz":-420},"id":"PDVYfvkHrKzs"},"outputs":[],"source":["def resizePadding(img, width, height):\n","    desired_w, desired_h = width, height #(width, height)\n","    _,img_h, img_w = img.shape  # old_size[0] is in (width, height) format\n","    # print(\"img_w: {0}, img_h: {1}\".format(img_w, img_h))\n","    # ratio = img_w/float(img_h)\n","    # print(\"ratio:\", ratio)\n","    # new_w = int(desired_h*ratio)\n","    # new_w = new_w if desired_w == None else min(desired_w, new_w)\n","    # img = img.resize((3, desired_h, new_w), Image.ANTIALIAS)\n","\n","    # padding image\n","    img = img.permute(1,2,0)\n","    img = img.numpy()\n","    img = img*255.0\n","    img = Image.fromarray(img.astype('uint8'), mode = \"RGB\")\n","    if desired_w != None: # and desired_w > new_w:\n","        new_img = Image.new(\"RGB\", (desired_w, desired_h), color = 255)\n","        new_img.paste(img,(0,0))\n","        img = new_img\n","\n","    img = ToTensor()(img)\n","\n","    return img\n"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1655459890974,"user":{"displayName":"Quốc Hưng Vũ","userId":"18294274047591865356"},"user_tz":-420},"id":"whzaJwGbsa0I"},"outputs":[],"source":["class alignCollate(object):\n","\n","    def __init__(self, imgW, imgH):\n","        self.imgH = imgH\n","        self.imgW = imgW\n","    \n","    def __call__(self, batch):\n","        images, label_y, label, y_string= zip(*batch)\n","        imgH = self.imgH\n","        imgW = self.imgW\n","        images = [resizePadding(image, self.imgW, self.imgH) for image in images]\n","        images = torch.cat([t.unsqueeze(0) for t in images], 0)\n","        label_ynew = label_y[0].unsqueeze(0)\n","        for i,ts in enumerate(label_y):\n","            if i != 0:\n","                label_ynew = torch.cat((label_ynew,ts.unsqueeze(0)),0)\n","        label_new = label[0].unsqueeze(0)\n","        for i,ts in enumerate(label):\n","            if i != 0:\n","                label_new = torch.cat((label_new,ts.unsqueeze(0)),0)\n","        return images, label_ynew, label_new, y_string"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":86752,"status":"ok","timestamp":1655460142590,"user":{"displayName":"Quốc Hưng Vũ","userId":"18294274047591865356"},"user_tz":-420},"id":"RRSezIRHrNQC","outputId":"d6526d9d-e992-4329-e6cb-0d6bd006981a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loadtime:  1.595595359802246\n"]}],"source":["st = time.time()\n","list_train = os.listdir('../train')\n","print(\"Loadtime: \", time.time()-st)\n","list_traindir = []\n","for index,image in enumerate(list_train):\n","    if data_preprocessing.valid_image(data_preprocessing.get_label(image)):\n","        list_traindir.append(image)\n","\n","\n","list_test = os.listdir('../test')\n","list_testdir = []\n","for index,image in enumerate(list_test):\n","    if data_preprocessing.valid_image(data_preprocessing.get_label(image)):\n","        list_testdir.append(image)"]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1655460142590,"user":{"displayName":"Quốc Hưng Vũ","userId":"18294274047591865356"},"user_tz":-420},"id":"XllgMIeUtfaJ"},"outputs":[],"source":["train_dataset = MyDataset(list_traindir[:70], cate = 'train')\n","test_dataset = MyDataset(list_testdir[:70], cate = 'test')"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1655460142590,"user":{"displayName":"Quốc Hưng Vũ","userId":"18294274047591865356"},"user_tz":-420},"id":"fzu0YfQHtGBo","outputId":"107a6cd7-297d-4917-866a-17ac311dd933"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train dataset length: 70\n","Test dataset length: 70\n"]}],"source":["print(\"Train dataset length: {0}\".format(len(train_dataset)))\n","print(\"Test dataset length: {0}\".format(len(test_dataset)))"]},{"cell_type":"code","execution_count":44,"metadata":{"executionInfo":{"elapsed":401,"status":"ok","timestamp":1655460421190,"user":{"displayName":"Quốc Hưng Vũ","userId":"18294274047591865356"},"user_tz":-420},"id":"jJAJhe5E3iUd"},"outputs":[],"source":["# batch_size = 64\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=20, shuffle=False, num_workers=0,drop_last = False, collate_fn = alignCollate(350,32))\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=20, shuffle=False, num_workers=0, drop_last = False, collate_fn = alignCollate(350,32))"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["def greedy_decode(src, model, src_mask, max_len=32, start_symbol=1):\n","    memory = model.encode(src, src_mask)\n","    ys = torch.ones(1, 1).fill_(start_symbol).long().to(device)\n","    for i in range(max_len-1):\n","        out = model.decode(memory, src_mask, \n","                           Variable(ys), \n","                           Variable(subsequent_mask(ys.size(1))\n","                                    .long().to(device)))\n","        prob = model.generator(out[:, -1])\n","        _, next_word = torch.max(prob, dim = 1)\n","        next_word = next_word.data[0]\n","        ys = torch.cat([ys, \n","                        torch.ones(1, 1).long().to(device).fill_(next_word)], dim=1)\n","        if token2char[next_word.item()] == '>':\n","            break\n","    ret = ys.cpu().numpy()[0]\n","    out = [token2char[i] for i in ret]\n","    out = \"\".join(out[1:-1])\n","    return out\n"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["def run_epoch(dataloader, model, loss_compute):\n","    \"Standard Training and Logging Function\"\n","    total_tokens = 0\n","    total_loss = 0\n","    tokens = 0\n","    size = len(dataloader.dataset)\n","    print(\"Size:\", size)\n","    sum_acc = 0\n","    for bat, (imgs, labels_y, labels, y_string) in enumerate(tqdm(dataloader)):\n","        n_acc_sentence = 0\n","        batch = Batch(imgs, labels_y, labels)\n","        out = model(batch.imgs, batch.trg, batch.src_mask, batch.trg_mask)\n","        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n","        total_loss += loss\n","        total_tokens += batch.ntokens\n","        tokens += batch.ntokens\n","        pred = model.generator(out)\n","        pred = pred.argmax(2)\n","        print(pred[0])\n","        print(labels_y[0])\n","        for i in range(20):\n","            sen_i = labels_y[i]\n","            n_acc_sentence += 1\n","            for c in range(32):\n","                if sen_i[c] == 65:\n","                    break\n","                elif (pred[i][c] != sen_i[c]):\n","                    n_acc_sentence -= 1\n","                    break\n","        sum_acc += n_acc_sentence\n","    print(\"Number of acc: \",sum_acc)\n","\n","    return total_loss / total_tokens , sum_acc/float(size)"]},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":560},"executionInfo":{"elapsed":513834,"status":"error","timestamp":1655460936322,"user":{"displayName":"Quốc Hưng Vũ","userId":"18294274047591865356"},"user_tz":-420},"id":"IYl7HCvUqxMv","outputId":"1ae3ec5f-5b15-43fe-c1ad-582db4251733"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\HP\\Desktop\\Transformer-OCR\\model.py:254: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n","  nn.init.xavier_uniform(p)\n","c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 1\n","Size: 70\n"]},{"name":"stderr","output_type":"stream","text":[" 25%|██▌       | 1/4 [00:03<00:09,  3.17s/it]"]},{"name":"stdout","output_type":"stream","text":["tensor([21, 25,  9, 36, 38, 22,  5, 60, 60, 13, 54, 39, 15, 60, 60, 60, 60, 60,\n","        25, 39, 60, 60, 22, 30, 30,  4, 30, 60, 60, 25, 60, 25])\n","tensor([ 3,  3,  2,  4, 29, 50,  7, 64, 33,  8, 38, 56, 65,  0,  0,  0,  0,  0,\n","         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n","       dtype=torch.int32)\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 2/4 [00:06<00:06,  3.27s/it]"]},{"name":"stdout","output_type":"stream","text":["tensor([11, 18, 59, 12, 59, 59, 54, 56,  9, 41, 20, 60, 60, 59, 60, 59, 60, 30,\n","        25, 60, 23, 60, 60, 60, 60, 60, 25, 30, 18, 24, 48, 18])\n","tensor([ 3,  5,  5, 17,  6, 13, 38, 29, 52, 61,  2, 58, 48, 11, 29, 65,  0,  0,\n","         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n","       dtype=torch.int32)\n"]},{"name":"stderr","output_type":"stream","text":[" 75%|███████▌  | 3/4 [00:10<00:03,  3.64s/it]"]},{"name":"stdout","output_type":"stream","text":["tensor([59, 59, 59, 36, 18, 42, 52, 54, 12, 54, 14, 36, 18, 22, 12, 22, 50, 50,\n","        23, 23,  8, 30, 18, 39, 60, 60, 60, 23, 38, 60, 39, 60])\n","tensor([ 3,  7,  4, 53, 40, 22, 34, 22,  8, 43,  2, 27, 29, 40, 29, 44, 64, 47,\n","        47, 34, 65,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n","       dtype=torch.int32)\n"]},{"name":"stderr","output_type":"stream","text":[" 75%|███████▌  | 3/4 [00:12<00:04,  4.30s/it]"]},{"name":"stdout","output_type":"stream","text":["tensor([45, 30, 62,  9, 62, 59, 38, 36,  4, 33, 36, 36, 23,  5, 60, 18, 18, 60,\n","        38, 60, 30, 60, 38, 38, 60, 60, 18, 30, 60,  4, 60, 60])\n","tensor([ 3,  8, 57, 56, 62, 59, 23, 38, 29, 64,  2, 33, 42, 65,  0,  0,  0,  0,\n","         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n","       dtype=torch.int32)\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"ename":"IndexError","evalue":"index 10 is out of bounds for dimension 0 with size 10","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32mc:\\Users\\HP\\Desktop\\Transformer-OCR\\testTransformerOCR.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/Transformer-OCR/testTransformerOCR.ipynb#ch0000015?line=8'>9</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpoch:\u001b[39m\u001b[39m\"\u001b[39m, epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/Transformer-OCR/testTransformerOCR.ipynb#ch0000015?line=9'>10</a>\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/Transformer-OCR/testTransformerOCR.ipynb#ch0000015?line=10'>11</a>\u001b[0m     train_loss, acc \u001b[39m=\u001b[39m run_epoch(train_loader, model, SimpleLossCompute(model\u001b[39m.\u001b[39;49mgenerator, criterion, model_opt))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/Transformer-OCR/testTransformerOCR.ipynb#ch0000015?line=11'>12</a>\u001b[0m \u001b[39m#     model.eval()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/Transformer-OCR/testTransformerOCR.ipynb#ch0000015?line=12'>13</a>\u001b[0m \u001b[39m#     test_loss = run_epoch(test_loader, model, SimpleLossCompute(model.generator, criterion, None))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/Transformer-OCR/testTransformerOCR.ipynb#ch0000015?line=13'>14</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTrain_loss: \u001b[39m\u001b[39m\"\u001b[39m, train_loss\u001b[39m.\u001b[39mitem())\n","\u001b[1;32mc:\\Users\\HP\\Desktop\\Transformer-OCR\\testTransformerOCR.ipynb Cell 15'\u001b[0m in \u001b[0;36mrun_epoch\u001b[1;34m(dataloader, model, loss_compute)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/Transformer-OCR/testTransformerOCR.ipynb#ch0000014?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(labels_y[\u001b[39m0\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/Transformer-OCR/testTransformerOCR.ipynb#ch0000014?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m20\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/Transformer-OCR/testTransformerOCR.ipynb#ch0000014?line=21'>22</a>\u001b[0m     sen_i \u001b[39m=\u001b[39m labels_y[i]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/Transformer-OCR/testTransformerOCR.ipynb#ch0000014?line=22'>23</a>\u001b[0m     n_acc_sentence \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/Transformer-OCR/testTransformerOCR.ipynb#ch0000014?line=23'>24</a>\u001b[0m     \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m32\u001b[39m):\n","\u001b[1;31mIndexError\u001b[0m: index 10 is out of bounds for dimension 0 with size 10"]}],"source":["model = make_model(len(char2token))\n","# model.load_state_dict(torch.load('your-pretrain-model-path'))\n","model.to(device)\n","criterion = LabelSmoothing(size=len(char2token), padding_idx=0, smoothing=0.1)\n","criterion.cuda()\n","model_opt = NoamOpt(model.tgt_embed[0].d_model, 1, 2000,\n","        torch.optim.Adam(model.parameters(), lr=5e-2))\n","for epoch in range(100):\n","    print(\"Epoch:\", epoch+1)\n","    model.train()\n","    train_loss, acc = run_epoch(train_loader, model, SimpleLossCompute(model.generator, criterion, model_opt))\n","#     model.eval()\n","#     test_loss = run_epoch(test_loader, model, SimpleLossCompute(model.generator, criterion, None))\n","    print(\"Train_loss: \", train_loss.item())\n","    print(\"Accuracy: \", acc)\n","#     print(\"Test_loss\", test_loss.item())\n"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":12,"status":"aborted","timestamp":1655460011595,"user":{"displayName":"Quốc Hưng Vũ","userId":"18294274047591865356"},"user_tz":-420},"id":"NiwnS-Q19lYv"},"outputs":[],"source":["# torch.save(model.state_dict(),\"testmodel.pth\")"]},{"cell_type":"code","execution_count":51,"metadata":{"executionInfo":{"elapsed":12,"status":"aborted","timestamp":1655460011595,"user":{"displayName":"Quốc Hưng Vũ","userId":"18294274047591865356"},"user_tz":-420},"id":"S-VHTpS_r8fQ"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\HP\\Desktop\\Transformer-OCR\\model.py:254: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n","  nn.init.xavier_uniform(p)\n"]},{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["models = make_model(len(char2token))\n","models.load_state_dict(torch.load(\"model40epoch.pth\", map_location=\"cpu\"))"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([30, 40, 43, 47, 24, 58, 41, 47,  2, 61, 53, 49, 63, 19, 26, 11,  3, 65,\n","        65, 65, 65, 65, 65, 65, 65, 61, 65, 65, 65, 65, 65, 65])\n"]}],"source":["models.to(device)\n","imgs, labels_y, labels, y_string = next(iter(train_loader))\n","batch = Batch(imgs, labels_y, labels)\n","out = models(batch.imgs, batch.trg, batch.src_mask, batch.trg_mask)\n","out = models.generator(out)\n","print(out.argmax(2)[0])"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":13,"status":"aborted","timestamp":1655460011596,"user":{"displayName":"Quốc Hưng Vũ","userId":"18294274047591865356"},"user_tz":-420},"id":"xk-mzjMeAoPU"},"outputs":[],"source":["img_transforms = transforms.Compose([\n","    transforms.ToTensor(),\n","    # transforms.Resize((350,32))\n","])"]},{"cell_type":"code","execution_count":86,"metadata":{"executionInfo":{"elapsed":13,"status":"aborted","timestamp":1655460011596,"user":{"displayName":"Quốc Hưng Vũ","userId":"18294274047591865356"},"user_tz":-420},"id":"DKazqmu9_r9_"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1, 1, 256])\n","torch.Size([1, 2, 256])\n","torch.Size([1, 3, 256])\n","torch.Size([1, 4, 256])\n","torch.Size([1, 5, 256])\n","torch.Size([1, 6, 256])\n","torch.Size([1, 7, 256])\n","torch.Size([1, 8, 256])\n","torch.Size([1, 9, 256])\n","torch.Size([1, 10, 256])\n","torch.Size([1, 11, 256])\n","torch.Size([1, 12, 256])\n","torch.Size([1, 13, 256])\n","torch.Size([1, 14, 256])\n","torch.Size([1, 15, 256])\n","torch.Size([1, 16, 256])\n","torch.Size([1, 17, 256])\n","torch.Size([1, 18, 256])\n","torch.Size([1, 19, 256])\n","torch.Size([1, 20, 256])\n","torch.Size([1, 21, 256])\n","Pred is:  03B1H7Ke6 9DfVO8ijqp\n"]}],"source":["models.to(device)\n","models.eval()\n","src_mask=Variable(torch.from_numpy(np.ones([1, 1, 44], dtype=np.bool)).to(device))\n","img = Image.open('../test/03B1H7Ke6 9DfVO8ijqp_2009.jpg')\n","img = img_transforms(img)\n","img = resizePadding(img,350,32)\n","img = img.unsqueeze(0).to(device)\n","pred = greedy_decode(img,model = models, src_mask=src_mask)\n","print(\"Pred is: \",pred)"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[],"source":["# def val(models,cate):\n","#   print('Start validation !')\n","#   acc = 0\n","#   size = len(list_testdir)\n","#   size = 10\n","#   for img_dir in list_testdir[:10]:\n","#     img_dirs = '../'+ cate + '/' + img_dir \n","#     img = Image.open(img_dirs)\n","#     img = img_transforms(img)\n","#     img = resizePadding(img,350,32)\n","#     img = img.unsqueeze(0).to(device)\n","#     print(img.shape)\n","#     pred = greedy_decode(img,model = models, src_mask=src_mask)\n","#     label = data_preprocessing.get_label(img_dir)\n","#     if pred == label:\n","#       acc += 1\n","#     # print(\"Pred is: \",pred)\n","#     # print(\"Label is: \",data_preprocessing.get_label(img_dir))\n","#   print(\"Accuracy: \",acc/float(size))"]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[],"source":["def val(dataloader, models):\n","  print('Start validation !')\n","  size = len(dataloader.dataset)\n","  models.eval()\n","  correct = 0\n","  with torch.no_grad():\n","    for batch, (imgs, label_y, label, label_ystring) in enumerate(tqdm(dataloader)):\n","        for i in range(imgs.size(0)):\n","            img = imgs[i].unsqueeze(0).to(device)\n","            src_mask=Variable(torch.from_numpy(np.ones([1, 1, 44], dtype=np.bool)).to(device))\n","            pred = greedy_decode(img,model = models, src_mask=src_mask)\n","            batchs_size = img.size(0)\n","            for pred, target in zip(pred, label_ystring):\n","                if pred == target:\n","                    correct += 1\n","        accuracy = correct/float(size)\n","    print(\"Accuracy: {0}\".format(accuracy))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["val(test_loader,models)"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Start validation !\n"]},{"ename":"AttributeError","evalue":"'EncoderDecoder' object has no attribute 'dataset'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32mc:\\Users\\HP\\Desktop\\Transformer-OCR\\testTransformerOCR.ipynb Cell 24'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/Transformer-OCR/testTransformerOCR.ipynb#ch0000023?line=0'>1</a>\u001b[0m val(models,\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m)\n","\u001b[1;32mc:\\Users\\HP\\Desktop\\Transformer-OCR\\testTransformerOCR.ipynb Cell 22'\u001b[0m in \u001b[0;36mval\u001b[1;34m(dataloader, models)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/Transformer-OCR/testTransformerOCR.ipynb#ch0000021?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mval\u001b[39m(dataloader, models):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/Transformer-OCR/testTransformerOCR.ipynb#ch0000021?line=1'>2</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mStart validation !\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/Transformer-OCR/testTransformerOCR.ipynb#ch0000021?line=2'>3</a>\u001b[0m   size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(dataloader\u001b[39m.\u001b[39;49mdataset)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/Transformer-OCR/testTransformerOCR.ipynb#ch0000021?line=3'>4</a>\u001b[0m   models\u001b[39m.\u001b[39meval()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/Transformer-OCR/testTransformerOCR.ipynb#ch0000021?line=4'>5</a>\u001b[0m   correct \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n","File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1185\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1183\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1184\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1185\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1186\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n","\u001b[1;31mAttributeError\u001b[0m: 'EncoderDecoder' object has no attribute 'dataset'"]}],"source":["val(models,'train')"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from torch.utils.tensorboard import SummaryWriter\n","writer = SummaryWriter('runs')"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[]\n"]}],"source":["accuracy = []\n","for i in range(50):\n","    writer.add_scalar(\"Training Loss \", i, global_step=i)\n","print(accuracy)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["^C\n"]}],"source":["!tensorboard --logdir runs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMCifQbzGTLJoVam2a35p1k","collapsed_sections":[],"name":"testTransformerOCR.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.8.4 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.4"},"vscode":{"interpreter":{"hash":"b337b16e1f284c9fe7de692799556d56c1809887abe3f5a49ffeb9e7df151cfb"}}},"nbformat":4,"nbformat_minor":0}
